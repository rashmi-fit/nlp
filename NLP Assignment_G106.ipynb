{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe4e602",
   "metadata": {},
   "source": [
    "# NLP Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13297764",
   "metadata": {},
   "source": [
    "Project Name: Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d713dc",
   "metadata": {},
   "source": [
    "Link to the Dataset:  https://www.kaggle.com/arbazkhan971/analyticvidhyadatasetsentiment?select=train_F3WbcTw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82cd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wordcloud\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6230d05b",
   "metadata": {},
   "source": [
    "# Download the file and set it as a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78878e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading both Train and Test dataset\n",
    "df_train = pd.read_csv('train_F3WbcTw.csv')\n",
    "df_test = pd.read_csv('test_tOlRoBf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0f362",
   "metadata": {},
   "source": [
    "### Checking the data inside both Train and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ddf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56543e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f1342",
   "metadata": {},
   "source": [
    "# Convert the text to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textLower(text):\n",
    "# Convert the text to lower case.\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(textLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adccf65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(textLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'].iloc[0] #checking lowercase text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e37d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'].iloc[0] #checking lowercase text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4060c",
   "metadata": {},
   "source": [
    "### Tokenise Train and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d79d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the train data\n",
    "# nltk.download(\"punkt\")\n",
    "df_train['tokenized_text'] = df_train['text'].apply(word_tokenize)\n",
    "print (\"Tokenized Text for Train dataset: \\n\")\n",
    "df_train['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the test data\n",
    "df_test['tokenized_text'] = df_test['text'].apply(word_tokenize)\n",
    "print (\"Tokenized Text for Test dataset: \\n\")\n",
    "df_test['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe93da5",
   "metadata": {},
   "source": [
    "# Remove punctuations, special characters and stopwords from the text column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29871b2",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f83ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extarcted a list of stopwords in Python NLTK\n",
    "from nltk.corpus import stopwords\n",
    "list1_stopWords = stopwords.words('english')\n",
    "with open('stopwords.txt','r') as file: #extracted a stopwords list file from kaggle\n",
    "    stopwords = file.read().splitlines()\n",
    "    list2_stopWords = stopwords\n",
    "total_stopWords = list1_stopWords + list2_stopWords\n",
    "stop_words = list(set(total_stopWords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09878559",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stopwords from both Train and Test dataset\n",
    "def remove_SW(x):\n",
    "    words = word_tokenize(x)\n",
    "    wordsFiltered = list(set(words) - set(stop_words))\n",
    "    return ' '.join(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(lambda x: remove_SW(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(lambda x: remove_SW(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c323717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'].iloc[0] #checking if the stopwords was removed from Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6926853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'].iloc[0] #checking if the stopwords was removed from Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8d53c",
   "metadata": {},
   "source": [
    "### Remove Punctuations and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuations from text\n",
    "def remove_PuncChars(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f40fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(remove_PuncChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(remove_PuncChars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6091a0",
   "metadata": {},
   "source": [
    "### Cleaning more data inside datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing Emails\n",
    "def removEmails(x):\n",
    "    pattern=r'\\S+@\\S+'\n",
    "    x=re.sub(pattern,'',x)\n",
    "    return x \n",
    "\n",
    "# #Removing URLs\n",
    "def removeURL(x):\n",
    "    pattern=r'http\\S+\\Swww\\S+org\\S'\n",
    "    x=re.sub(pattern,'',x)\n",
    "    return x\n",
    "\n",
    "# #Removing html strips\n",
    "def stripHTML(x):\n",
    "    soup = BeautifulSoup(x, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# #Removing the square brackets\n",
    "def removeSqBrackets(x):\n",
    "    return re.sub('\\[[^]]*\\]', '', x)\n",
    "\n",
    "# #Removing the noisy text\n",
    "def noisyText(x):\n",
    "    x = stripHTML(x)\n",
    "    x = removeSqBrackets(x)\n",
    "    return x\n",
    "\n",
    "# #Removing numbers\n",
    "def removeNum(x):\n",
    "    pattern = r'\\d+'\n",
    "    x = re.sub(pattern, '', x)\n",
    "    return x\n",
    "\n",
    "# #Remove emojis\n",
    "def removEmojis(x):\n",
    "    emojiPattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"\\U0001f926-\\U0001f937\"\n",
    "                               \"\\U00010000-\\U0010ffff\"\n",
    "                               \"\\u2640-\\u2642\"\n",
    "                               \"\\u2600-\\u2B55\"\n",
    "                               \"\\u200d\"\n",
    "                               \"\\u23cf\"\n",
    "                               \"\\u23e9\"\n",
    "                               \"\\u231a\"\n",
    "                               \"\\ufe0f\"  # dingbats\n",
    "                               \"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    # Remove emojis from the text\n",
    "    x = emojiPattern.sub(r'', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(x):\n",
    "    x = removEmails(x)\n",
    "    x = removeURL(x)\n",
    "    x = stripHTML(x)\n",
    "    x = removeSqBrackets(x)\n",
    "    x = noisyText(x)\n",
    "    x = removeNum(x)\n",
    "    x = removEmojis(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db0a78",
   "metadata": {},
   "source": [
    "# Create two objects X and y. X will be the ' Text ' column dataframe and y will be the “Sentiment” column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad258a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename text column\n",
    "df_train.rename(columns={'text':'Text'},inplace = True)\n",
    "df_test.rename(columns={'text':'Text'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f294d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename sentiment column\n",
    "df_train.rename(columns={'sentiment':'Sentiment'},inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b1399",
   "metadata": {},
   "source": [
    "# Use TF-IDF and  CountVectorizer for word embeddings on the ‘Text’ column, display the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "def findWordEmbeddings(x):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    tfidf_embeddings = tfidf_vectorizer.fit_transform(x)\n",
    "    count_embeddings = count_vectorizer.fit_transform(x)\n",
    "    \n",
    "    tfidf_embeddings_array = tfidf_embeddings.toarray()\n",
    "    count_embeddings_array = count_embeddings.toarray()\n",
    "    \n",
    "    print('TF-IDF Embedding:\\n')\n",
    "    print(tfidf_embeddings_array)\n",
    "    print('\\nCount Embedding:\\n')\n",
    "    print(count_embeddings_array)\n",
    "    print('\\nUnique TF-IDF Embeddings:\\n')\n",
    "    print(np.unique(tfidf_embeddings.toarray()))\n",
    "    print('\\nUnique Count Embeddings:\\n')\n",
    "    print(np.unique(count_embeddings.toarray()))\n",
    "    # print('\\n Unique words that were encountered during the fitting process of the TfidfVectorizer:\\n')\n",
    "    # print(tfidf_vectorizer.vocabulary_)\n",
    "    return tfidf_embeddings_array, count_embeddings_array\n",
    "\n",
    "#Performing TF-IDF and Count on 10 samples of Train dataset\n",
    "x_train = df_train['Text']\n",
    "tfidf_embeddings, count_embeddings = findWordEmbeddings(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2735cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing TF-IDF and Count on 10 samples of Test dataset\n",
    "x_test = df_test['Text']\n",
    "tfidf_embeddings, count_embeddings = findWordEmbeddings(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96377ad",
   "metadata": {},
   "source": [
    "- When we have a very large dataset, particularly when using TF-IDF vectorization, the resulting matrix becomes sparse, meaning that most of the values are zeros. \n",
    "- When visualizing this matrix using PCA (Principal Component Analysis) with two components, the points representing the words are scattered all over the 2D plane of the graph. This makes it difficult to understand the relationships between the words.\n",
    "\n",
    "- To address this issue and improve the visualization, t-SNE (t-Distributed Stochastic Neighbor Embedding) is often used. t-SNE helps arrange the points in a way that groups similar words together on the graph. \n",
    "- In other words, words that are clustered together on the graph are more likely to have similarities based on the selected features. \n",
    "- These similarities can be interpreted as semantic similarities, indicating that the words are related in meaning or context. \n",
    "- By using t-SNE, the visualization becomes more informative and facilitates the identification of word groups and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6c574",
   "metadata": {},
   "source": [
    "# Generate embeddings using CBOW and Skip-gram on the text using three different window sizes, display the embeddings using a visualization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e759b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddab152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the clean text column in Train set\n",
    "df_train['tokenized_text_clean'] = df_train['Text'].apply(word_tokenize)\n",
    "print (\"Tokenized Clean Text for Train dataset: \\n\")\n",
    "df_train['tokenized_text_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the clean text column in Test set\n",
    "df_test['tokenized_text_clean'] = df_test['Text'].apply(word_tokenize)\n",
    "print (\"Tokenized Clean Text for Test dataset: \\n\")\n",
    "df_test['tokenized_text_clean'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Function to generate model (CBOW/Skipgram) using TSNE followed by PCA\n",
    "def getEmbeddings(corpus:pd.Series, window, title, skipgram:bool):\n",
    "    if skipgram:\n",
    "        print(\"Creating SkipGram model\")\n",
    "        sg = 1\n",
    "    else:\n",
    "        print(\"Creating CBOW model\")\n",
    "        sg = 0\n",
    "    corpus = corpus.tolist()\n",
    "    embedding_model = Word2Vec(sentences=corpus, window=window, sg=sg, min_count=2)\n",
    "    words_list = list(embedding_model.wv.key_to_index.keys())\n",
    "    word_vectors = [embedding_model.wv.get_vector(word) for word in words_list]\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    word_embeddings_2d = pca.fit_transform(word_vectors)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=10, random_state=42)\n",
    "    word_vec = tsne.fit_transform(word_embeddings_2d)\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(word_vec[:, 0], word_vec[:, 1], c='b', alpha=0.6)\n",
    "    for i, word in enumerate(words_list):\n",
    "        plt.annotate(word, alpha=0.5, xy=(word_vec[i, 0], word_vec[i, 1]), xytext=(5, 2),\n",
    "                     textcoords='offset points', ha='right', va='bottom', size=4)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7df706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate model (CBOW/Skipgram) using only PCA\n",
    "def get_pca_Embeddings(corpus:pd.Series, window, title, skipgram:bool):\n",
    "    if skipgram:\n",
    "        print(\"Creating SkipGram model\")\n",
    "        sg = 1\n",
    "    else:\n",
    "        print(\"Creating CBOW model\")\n",
    "        sg = 0\n",
    "    corpus = corpus.tolist()\n",
    "    embedding_model = Word2Vec(sentences=corpus, window=window, sg=sg, min_count=2)\n",
    "    words_list = list(embedding_model.wv.key_to_index.keys())\n",
    "    word_vectors = [embedding_model.wv.get_vector(word) for word in words_list]\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    word_embeddings_2d = pca.fit_transform(word_vectors)\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.scatter(word_embeddings_2d[:, 0], word_embeddings_2d[:, 1], c='b', alpha=0.6)\n",
    "    for i, word in enumerate(words_list):\n",
    "        plt.annotate(word, alpha=0.5, xy=(word_embeddings_2d[i, 0], word_embeddings_2d[i, 1]), xytext=(5, 2),\n",
    "                     textcoords='offset points', ha='right', va='bottom', size=4)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b222fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating SkipGram model using TSNE followed by PCA by iterating over window size of 5,7,9\n",
    "# corpus = df_train['tokenized_text_clean']\n",
    "# sg_models = []\n",
    "# for i in range(5,10,2):\n",
    "#     model = getEmbeddings(corpus, window=i, title=f\"SkipGram - windowSize {i}\", skipgram=True)\n",
    "#     sg_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating CBOW model using TSNE followed by PCA by iterating over window size of 5,7,9\n",
    "corpus = df_train['tokenized_text_clean']\n",
    "cbow_models = []\n",
    "for i in range(5,10,2):\n",
    "    model = getEmbeddings(corpus, window=i, title=f\"CBOW - windowSize {i}\", skipgram=False)\n",
    "    cbow_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating CBOW model using PCA by iterating over window size of 5,7,9\n",
    "corpus = df_train['tokenized_text_clean']\n",
    "cbow_models_pca = []\n",
    "for i in range(5,10,2):\n",
    "    model = get_pca_Embeddings(corpus, window=i, title=f\"CBOW - windowSize {i}\", skipgram=False)\n",
    "    cbow_models_pca.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating SkipGram model using PCA by iterating over window size of 5,7,9\n",
    "# corpus = df_train['tokenized_text_clean']\n",
    "# sg_models_pca = []\n",
    "# for i in range(5,10,2):\n",
    "#     model = get_pca_Embeddings(corpus, window=i, title=f\"SkipGram - windowSize {i}\", skipgram=True)\n",
    "#     sg_models_pca.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating word vectors for each word from the CBOW model\n",
    "for i in cbow_models:\n",
    "    words = list(i.wv.key_to_index.keys())\n",
    "    word_vectors = [i.wv.get_vector(word) for word in words]\n",
    "    print(f\"Model {i}\",word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d53935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating word vectors for each word from the SkipGram model\n",
    "for i in sg_models:\n",
    "    words = list(i.wv.key_to_index.keys())\n",
    "    word_vectors = [i.wv.get_vector(word) for word in words]\n",
    "    print(f\"Model {i}\",word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d53935",
   "metadata": {},
   "source": [
    "## CBOW (Continuous Bag of Words Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and building vocab for each model of CBOW with TSNE followed by PCA\n",
    "for i, model in enumerate(cbow_models):\n",
    "    model.build_vocab(df_train['tokenized_text_clean'])\n",
    "    model.train(df_train['tokenized_text_clean'], \n",
    "                total_examples=model.corpus_count, \n",
    "                epochs=5)\n",
    "    print(f\"Vocabulary for CBOW model - {i}\",model.wv.index_to_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a691c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and building vocab for each model of CBOW with PCA\n",
    "for i, model in enumerate(cbow_models_pca):\n",
    "    model.build_vocab(df_train['tokenized_text_clean'])\n",
    "    model.train(df_train['tokenized_text_clean'], \n",
    "                total_examples=model.corpus_count, \n",
    "                epochs=5)\n",
    "    print(f\"Vocabulary for CBOW model - {i}\",model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and building vocab for each model of skipgram with TSNE followed by PCA\n",
    "for i, model in enumerate(sg_models):\n",
    "    model.build_vocab(df_train['tokenized_text_clean'])\n",
    "    model.train(df_train['tokenized_text_clean'], \n",
    "                total_examples=model.corpus_count, \n",
    "                epochs=5)\n",
    "    print(f\"Vocabulary for SkipGram model - {i}\",model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and building vocab for each model of skipgram with PCA only\n",
    "for i, model in enumerate(sg_models_pca):\n",
    "    model.build_vocab(df_train['tokenized_text_clean'])\n",
    "    model.train(df_train['tokenized_text_clean'], \n",
    "                total_examples=model.corpus_count, \n",
    "                epochs=5)\n",
    "    print(f\"Vocabulary for SkipGram model - {i}\",model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec0bc2",
   "metadata": {},
   "source": [
    " # Displaying the embeddings and Interpretting the results for predicting similar words to the most frequent word using the results of embeddings on different window sizes used in above task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding similar word embeddings for CBOW model using TSNE followed by PCA\n",
    "most_frequent_word = cbow_models[0].wv.index_to_key[0]\n",
    "similar_words = []\n",
    "window_sizes = [5,7,9]\n",
    "for model in cbow_models:\n",
    "    similar_words.append(model.wv.most_similar(most_frequent_word))\n",
    "\n",
    "for i, window_size in enumerate(window_sizes):\n",
    "    print(f\"Embeddings with Window Size {window_size}:\")\n",
    "    for word, similarity in similar_words[i]:\n",
    "        print(f\"- {word}: {similarity}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c104b",
   "metadata": {},
   "source": [
    "- The output of CBOW with TSNE and PCA shows that the words with the highest similarities are clustered together, regardless of the window size. For example, in the window size 5 output, the words \"wwwniceorguk\", \"mid\", and \"blog\" are all clustered together, as are the words \"del\", \"carcinomatosis\", and \"kisqali\". This suggests that these words are semantically related, even though they may not be adjacent to each other in the text.\n",
    "\n",
    "- The window size does have some impact on the clustering, however. With a larger window size, more words are included in the context window, which can lead to more subtle relationships being captured. For example, in the window size 9 output, the words \"genovese\", \"wwwniceorguk\", and \"display\" are clustered together, suggesting that they are all related to clinical trials.\n",
    "\n",
    "- Overall, the output of CBOW with TSNE and PCA shows that the model is able to learn meaningful relationships between words, even when those words are not adjacent to each other in the text. This can be useful for a variety of tasks, such as natural language understanding and machine translation.\n",
    "\n",
    "- Here are some additional inferences that can be made from the output:\n",
    "\n",
    "- The words \"wwwniceorguk\" and \"mid\" are often used in the context of healthcare, as they are both related to the National Institute for Health and Care Excellence (NICE).\n",
    "- The words \"blog\", \"strength\", and \"postpartum\" are often used in the context of women's health.\n",
    "- The words \"del\", \"carcinomatosis\", and \"kisqali\" are often used in the context of cancer treatment.\n",
    "- The words \"genovese\", \"wwwniceorguk\", and \"display\" are often used in the context of clinical trials.\n",
    "\n",
    "- These inferences can be used to improve the performance of natural language processing tasks, such as text classification and information retrieval. For example, if a text is about healthcare, the model can be more likely to classify it as such if it contains the words \"wwwniceorguk\" or \"mid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40385a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding similar word embeddings for CBOW model using PCA\n",
    "most_frequent_word = cbow_models_pca[0].wv.index_to_key[0]\n",
    "similar_words = []\n",
    "window_sizes = [5,7,9]\n",
    "for model in cbow_models_pca:\n",
    "    similar_words.append(model.wv.most_similar(most_frequent_word))\n",
    "\n",
    "for i, window_size in enumerate(window_sizes):\n",
    "    print(f\"Embeddings with Window Size {window_size}:\")\n",
    "    for word, similarity in similar_words[i]:\n",
    "        print(f\"- {word}: {similarity}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37240a8",
   "metadata": {},
   "source": [
    "- The output of the most similar words using CBOW with PCA shows that the words with the highest similarities are clustered together, regardless of the window size. For example, in the window size 5 output, the words \"count\", \"strength\", and \"result\" are all clustered together, as are the words \"healthcare\", \"nivolumab\", and \"atazanavir\". This suggests that these words are semantically related, even though they may not be adjacent to each other in the text.\n",
    "\n",
    "- The window size does have some impact on the clustering, however. With a larger window size, more words are included in the context window, which can lead to more subtle relationships being captured. For example, in the window size 9 output, the words \"intravitreous\", \"result\", and \"compulsory\" are clustered together, suggesting that they are all related to the medical field.\n",
    "\n",
    "- Overall, the output of the most similar words using CBOW with PCA shows that the model is able to learn meaningful relationships between words, even when those words are not adjacent to each other in the text. This can be useful for a variety of tasks, such as natural language understanding and machine translation.\n",
    "\n",
    "- Here are some additional inferences that can be made from the output:\n",
    "\n",
    "- The words \"count\", \"strength\", and \"result\" are often used in the context of clinical trials, as they are all related to the process of measuring the effectiveness of a treatment.\n",
    "- The words \"healthcare\", \"nivolumab\", and \"atazanavir\" are often used in the context of cancer treatment, as they are all related to the treatment of cancer.\n",
    "- The words \"intravitreous\", \"result\", and \"compulsory\" are often used in the context of eye care, as they are all related to the treatment of eye diseases.\n",
    "\n",
    "- These inferences can be used to improve the performance of natural language processing tasks, such as text classification and information retrieval. For example, if a text is about clinical trials, the model can be more likely to classify it as such if it contains the words \"count\", \"strength\", or \"result\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding similar word embeddings for SkipGram model using TSNE followed by PCA\n",
    "most_frequent_word = sg_models[0].wv.index_to_key[0]\n",
    "similar_words = []\n",
    "window_sizes = [5,7,9]\n",
    "for model in sg_models:\n",
    "    similar_words.append(model.wv.most_similar(most_frequent_word))\n",
    "\n",
    "for i, window_size in enumerate(window_sizes):\n",
    "    print(f\"Embeddings with Window Size {window_size}:\")\n",
    "    for word, similarity in similar_words[i]:\n",
    "        print(f\"- {word}: {similarity}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d538422",
   "metadata": {},
   "source": [
    "- The output of the Skipgram with TSNE and PCA shows that the words with the highest similarities are clustered together, regardless of the window size. For example, in the window size 5 output, the words \"meeting\", \"blocks\", and \"healthcare\" are all clustered together, as are the words \"care\", \"past\", and \"count\". This suggests that these words are semantically related, even though they may not be adjacent to each other in the text.\n",
    "\n",
    "- The window size does have some impact on the clustering, however. With a larger window size, more words are included in the context window, which can lead to more subtle relationships being captured. For example, in the window size 9 output, the words \"actually\", \"past\", and \"slowing\" are clustered together, suggesting that they are all related to the concept of time.\n",
    "\n",
    "- Overall, the output of the Skipgram with TSNE and PCA shows that the model is able to learn meaningful relationships between words, even when those words are not adjacent to each other in the text. This can be useful for a variety of tasks, such as natural language understanding and machine translation.\n",
    "\n",
    "- Here are some additional inferences that can be made from the output:\n",
    "\n",
    "- The words \"meeting\", \"blocks\", and \"healthcare\" are often used in the context of medical research or clinical trials, as they are all related to the process of conducting research.\n",
    "- The words \"care\", \"past\", and \"count\" are often used in the context of patient care, as they are all related to the process of providing medical services.\n",
    "- The words \"actually\", \"past\", and \"slowing\" are often used in the context of time, as they are all related to the concept of time.\n",
    "\n",
    "- These inferences can be used to improve the performance of natural language processing tasks, such as text classification and information retrieval. For example, if a text is about medical research, the model can be more likely to classify it as such if it contains the words \"meeting\", \"blocks\", or \"healthcare\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74adea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding similar word embeddings for SkipGram model using PCA\n",
    "most_frequent_word = sg_models_pca[0].wv.index_to_key[0]\n",
    "similar_words = []\n",
    "window_sizes = [5,7,9]\n",
    "for model in sg_models_pca:\n",
    "    similar_words.append(model.wv.most_similar(most_frequent_word))\n",
    "\n",
    "for i, window_size in enumerate(window_sizes):\n",
    "    print(f\"Embeddings with Window Size {window_size}:\")\n",
    "    for word, similarity in similar_words[i]:\n",
    "        print(f\"- {word}: {similarity}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e735e8",
   "metadata": {},
   "source": [
    "- The output of the most recent words of Skipgram with PCA shows that the words with the highest similarities are clustered together, regardless of the window size. For example, in the window size 5 output, the words \"result\", \"count\", and \"meeting\" are all clustered together, as are the words \"greater\", \"means\", and \"care\". This suggests that these words are semantically related, even though they may not be adjacent to each other in the text.\n",
    "\n",
    "- The window size does have some impact on the clustering, however. With a larger window size, more words are included in the context window, which can lead to more subtle relationships being captured. For example, in the window size 9 output, the words \"individual\", \"past\", and \"diagnosis\" are clustered together, suggesting that they are all related to the healthcare domain.\n",
    "\n",
    "- Overall, the output of the most recent words of Skipgram with PCA shows that the model is able to learn meaningful relationships between words, even when those words are not adjacent to each other in the text. This can be useful for a variety of tasks, such as natural language understanding and machine translation.\n",
    "\n",
    "- Here are some additional inferences that can be made from the output:\n",
    "\n",
    "- The words \"result\", \"count\", and \"meeting\" are often used in the context of medical research, as they are all related to the process of collecting and analyzing data.\n",
    "- The words \"greater\", \"means\", and \"care\" are often used in the context of healthcare, as they are all related to the provision of medical services.\n",
    "- The words \"individual\", \"past\", and \"diagnosis\" are often used in the context of patient care, as they are all related to the process of understanding and treating a patient's condition.\n",
    "\n",
    "- These inferences can be used to improve the performance of natural language processing tasks, such as text classification and information retrieval. For example, if a text is about medical research, the model can be more likely to classify it as such if it contains the words \"result\", \"count\", or \"meeting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf998e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
